<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <title>TaylorDiff.jl: Taking Higher-order Derivatives Seriously</title>
  <link rel="stylesheet" href="../reveal.js/dist/reveal.css" />
  <link rel="stylesheet" href="../reveal.js/dist/theme/academic.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.7.0/build/styles/xcode.min.css">
  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.2.0/dist/chart.umd.min.js"></script>
  <script type="module">
    import Reveal from "../reveal.js/dist/reveal.esm.js";
    import Markdown from "../reveal.js/plugin/markdown/markdown.esm.js";
    import Math from "../reveal.js/plugin/math/math.esm.js";
    import Highlight from "../reveal.js/plugin/highlight/highlight.esm.js"
    Reveal.initialize({
      plugins: [Markdown, Math.KaTeX, Highlight],
      width: 1920,
      height: 1080,
      center: false,
      margin: 0,
      display: "flex",
      transition: 'none',
      slideNumber: true,
      hash: true,
    });
  </script>
  <script type="module">

    Chart.defaults.aspectRatio = 1.5;
    Chart.defaults.font.size = 24;
    Chart.defaults.plugins.title.display = true;
    Chart.defaults.plugins.title.font.size = 36;
    const data = await (await fetch('https://benchmark.tansongchen.com/TaylorDiff.jl', { method: 'POST', body: JSON.stringify({}) })).json();
    const { tag, branch, benchmarkgroup } = data['TaylorDiff.jl#8c38fda4edf544e05ba4d97e9ddd5ced22d24a2f'];
    const {
      scalar: { forwarddiff: scalar_f, taylordiff: scalar_t },
      mlp: { forwarddiff: mlp_f, taylordiff: mlp_t },
      taylor_expansion: { taylordiff: te_t, taylorseries: te_s },
      pinn: { taylordiff: pinn_t, finitediff: pinn_f }
    } = benchmarkgroup;
    new Chart(document.getElementById('p1'), {
      type: 'line',
      data: {
        labels: Object.keys(scalar_f),
        datasets: [
          { data: Object.values(scalar_f).map(x => x.time), label: "ForwardDiff.jl" },
          { data: Object.values(scalar_t).map(x => x.time), label: "TaylorDiff.jl" }
        ]
      },
      options: {
        scales: {
          x: { title: { text: "Differentiation Order", display: true } },
          y: {
            title: { text: "Time / ns", display: true },
            type: 'logarithmic'
          }
        },
        plugins: { title: { text: "Scalar function derivatives", display: true } }
      }
    });
    new Chart(document.getElementById('p2'), {
      type: 'line',
      data: {
        labels: Object.keys(mlp_f),
        datasets: [
          { data: Object.values(mlp_f).map(x => x.time), label: "ForwardDiff.jl" },
          { data: Object.values(mlp_t).map(x => x.time), label: "TaylorDiff.jl" }
        ]
      },
      options: {
        scales: {
          x: { title: { text: "Differentiation Order", display: true } },
          y: {
            title: { text: "Time / ns", display: true },
            type: 'logarithmic'
          }
        },
        plugins: { title: { text: "Multi-layer Perceptron derivatives", display: true } }
      }
    });
    new Chart(document.getElementById('p3'), {
      type: 'bar',
      data: {
        labels: ['Time'],
        datasets: [
          { data: [te_s].map(x => x.time / 1000), label: "TaylorSeries.jl" },
          { data: [te_t].map(x => x.time / 1000), label: "TaylorDiff.jl" },
        ],
      },
      options: {
        scales: {
          y: {
            title: { text: "Time / μs", display: true },
          }
        },
        plugins: { title: { text: "Single variable Taylor expansion", display: true } }
      }
    });
    new Chart(document.getElementById('p4'), {
      type: 'bar',
      data: {
        labels: Object.keys(pinn_f),
        datasets: [
          { data: Object.values(pinn_f).map(x => x.time / 1000), label: "FiniteDifferences.jl" },
          { data: Object.values(pinn_t).map(x => x.time / 1000), label: "TaylorDiff.jl" },
        ],
      },
      options: {
        scales: {
          y: {
            title: { text: "Time / μs", display: true },
            // type: 'logarithmic',
          }
        },
        plugins: { title: { text: "PINN", display: true } }
      }
    })
  </script>
</head>

<body class="reveal">
  <main class="slides">
    <section class="hero">
      <h1>TaylorDiff.jl: Taking Higher-order Derivatives Seriously</h1>
      <p>Songchen Tan, Feb 2023</p>
    </section>
    <section class="hero">
      <h1>Introduction</h1>
    </section>
    <section data-markdown>
      <textarea data-template>
## Understanding higher-order AD

First-order AD is all about finding an efficient and maintainable way for executing the **chain rule formula**

$$
\frac{\mathrm df(g(x))}{\mathrm dx}=f'(g(x))g'(x)
$$

in which forward mode AD (based on operator overloading) achieve that by using **dual number arithmetic**.
---
Higher-order AD is, by analogy, finding an efficient and maintainable way for executing the **Faa di Bruno’s formula** ($\pi=(k_1,\cdots,k_n)$ is a partition of integer $n$, such that $\sum_iik_i=n$)

$$
\frac{\mathrm d^nf(g(x))}{\mathrm dx^n}=\sum_{\pi \in \Pi} f^{(|\pi|)}(g(x)) \cdot \prod_{B \in \pi} g^{(B)}(x)
$$

in which Taylor mode AD is a generalization of operator overloading forward mode AD, using **polynomial arithmetic**.

Faa di Bruno's formula itself didn't give much performance gain, since there are exponentially many partitions. However, it gives insights on how we could reuse computations across different order.
---
## Current solutions

### TaylorSeries.jl

- Capable for calculating the full derivative tensor for arbitrary order
- Operator-overloaded approach on one- and multi-variable Taylor polynomials
- For $n$-variable derivatives up to $k$ order, TaylorSeries.jl forces you to compute the full derivative, which takes $O(kn^k)$ time; however, **full derivative is rarely needed**
- Hard-coded higher-order rules, only a limited amount of primitives
- Dynamically-sized and mutating array, efficiency and compatibility problem

---
### Diffractor.jl

- Next-generation SCT-based forward-mode and reverse-mode AD, designed with support for higher-order derivatives in mind
- Has a direct implementation of Faa di Bruno's formula
- Relies on writing specific rules for efficiency
- Type inference issues

---
### `jax.experimental.jet`

- Python implementation of Taylor mode AD in JAX
- Can only compute higher-order directional derivatives
- Benefits from vectorized code
- Hard-coded higher-order rules (again)
---

## Design goals and non-goals

Let's combine the best part of them:

- Focus on directional derivative (that's where Taylor mode shines) as in `jax.jet`
    - Cross-derivatives achieved by nesting this AD (like $\partial^5 u/\partial x^2\partial y^3$)
- Order as a type parameter, as in Diffractor.jl
    - No performance penalty in lower-order
- Intuitive interface, as in TaylorSeries.jl
- Smarter fallbacks than generic Faa di Bruno's formula
    - By lifting ChainRules's `frule` to higher-order
- Composable with other ADs
      </textarea>
    </section>
    <section class="hero">
      <h1>Theory</h1>
    </section>
    <section data-markdown>
      <textarea data-template>
## Order as a type parameter

A Taylor polynomial is a order `N` polynomial represented by a wrapper over `NTuple{N + 1, P}` for arbitrary primal type `P`.

- Fully unrolled and non-allocating for performance
- No more mutating arrays issue

(You might be concerned about the fact that tangent space can be different from primal space, but that can be fixed by treating it separately like `P` and `NTuple{N, T}`. The mapping from primal space to tangent space is likely *idempotent*, so we won't have very different spaces for each order.)

---
## Taylor rules

For each function definition `f(x...) = Ω`, we make a overload `f(Tx...) = TΩ`, where

- `Tx` could be the primal type (optionally promoted to the polynomial type) or the polynomial type
- `TΩ` could be the primal type or the polynomial type

Overloading is per order for more aggressive optimization.

---
## Generation of Taylor rules from ChainRules `frule`s

How to obtain the Taylor rules automatically, so that we don't need to write them one-by-one? It turns out that we can make use of the higher-order product rule or division rule,

$$
f(g(x))^{(n)}=[f'(g(x))g'(x)]^{(n-1)}=\sum_{i=0}^{n-1}[f'(g(x))]^{(i)}g^{(n-i)}
$$

- Obtain $f'$ (often closely related to $f$ and $g$) from ChainRules
- Recurse to get the $n - 1$-th order derivative
- Raise the order via product rule or division rule

This process can be automated with SymbolicsUtils.jl.

---
## Complexity analysis

Let's assume that $f'(x)$ can be reconstructed from $f$ and $x$ via primitive operations ($+, -, \times, \div$) and finite mutual recursion.

- Each order of reconstruction is $O(n)$
- Each raising is $O(n)$
- Computing the Taylor polynomial is therefore at most $O(n^2)$
- For linear functions, it is in addition $O(n)$

---
Let $t = (t_0,\cdots,t_n)$ be a polynomial of order $n$, and $f=(f_0,\cdots,f_n)$ is the output polynomial of a function.

### Case study: $f = \tan t$

Let $s=(\tan t)'=1+\tan^2t=1+f^2$, compute in the following order:

$$
f_0, s_0\to f_1\to s_1\to f_2\to s_2\to\cdots \to f_n
$$

### Case study: $f = \arctan t$

Let $u=1/(\arctan t)'=1+t^2$, compute in the following order:

$$
u_0\to u_1\to\cdots\to u_{n-1}\to (f_0,\cdots,f_n)
$$

      </textarea>
    </section>
    <section class="hero">
      <h1>Implementation</h1>
      <p>Checkout the code at <a href="https://github.com/tansongchen/TaylorDiff.jl" target="_blank">TaylorDiff.jl</a>!</p>
    </section>
    <section data-markdown>
      <textarea data-template>
## Type definition and primitives

We define a new data structure `TaylorScalar` as described (it doesn't `<: Number` for technical reasons);

```julia
struct TaylorScalar{T,N}
    values::NTuple{N,T}
end
```

Taylor rules are unrolled by generated functions;
    
```julia
@generated function *(a::TS{T, N}, b::TS{T, N}) where {T, N}
    return quote
        va, vb = value(a), value(b)
        @inbounds TaylorScalar($([:(+($([:(va[$j] * vb[$(i + 1 - j)])
         for j in 1:i]...))) for i in 1:N]...))
    end
end
```
---
## Code generation

```julia
@eval @generated function (op::$F)(t::TaylorScalar{T, N}) where {T, N}
    der_expr = $(QuoteNode(toexpr(term)))
    f = $func
    quote
        $(Expr(:meta, :inline))
        t₁ = TaylorScalar{T, N - 1}(t)
        df = $der_expr
        $$raiser($f(value(t)[1]), df, t)
    end
end
```

---
## API

```julia
@inline function derivative(f, x::T, ::Val{N}) where {T, N}
    t = TaylorScalar{T, N}(x, one(x))
    return extract_derivative(f(t), N)
end

@inline function derivative(f, x::Vector{T}, l::Vector{T},
                            vN::Val{N}) where {T, N}
    t = map(TaylorScalar{T, N}, x, l)
    return extract_derivative(f(t), N)
end
```

---
## Downstream

Essentially some custom `rrule`s to avoid mutating code, e.g.

```julia
function rrule(::typeof(*), A::AbstractMatrix,
               t::AbstractVector{TaylorScalar{T, N}}) where {N, T}
    project_A = ProjectTo(A)
    function gemv_pullback(x̄)
        NoTangent(), @thunk(project_A(contract.(x̄, transpose(t)))), 
        @thunk(transpose(A)*x̄)
    end
    return A * t, gemv_pullback
end
```
      </textarea>
    </section>
    <section class="hero">
      <h1>Results</h1>
    </section>
    <section>
      <h2>Scenario 1: Scaling comparison</h2>
      <p>
        We compare the efficiency of TaylorDiff.jl to ForwardDiff.jl (nesting first-order derivative), with scalar function $f(x)=\sin(x)$  up to order 10 and a simple MLP $f(x)=W_2\cdot\exp.(W_1\cdot x+b_1)+b_2$ up to order 7:
      </p>
      <div style="display: flex">
        <div style="width: 1200px; margin: 0 auto" class="chart-container">
          <canvas id="p1"></canvas>
        </div>
        <div style="width: 1200px; margin: 0 auto" class="chart-container">
          <canvas id="p2"></canvas>
        </div>
      </div>
    </section>
    <section>
      <h2>Scenario 2: Taylor expansion</h2>
      <div style="width: 1200px; margin: 0 auto" class="chart-container">
        <canvas id="p3"></canvas>
      </div>
    </section>
    <section>
      <h2>Scenario 3: Physical model and optimizations</h2>
      <div style="width: 1200px; margin: 0 auto" class="chart-container">
        <canvas id="p4"></canvas>
      </div>
    </section>
    <section class="hero">
      <h1>Conclusion</h1>
    </section>
    <section data-markdown>
      <textarea data-template>
## Summary
- Taylor mode for linear scaling
- Generating Taylor rules from `frule`s
- No penalty of abstraction, performant even in low order
---
## Plans
- Add a "derivative backend" to NeuralPDE.jl
- Make Zygote happy with the generated higher-order rules
- OO has limitations. Go SCT or join Diffractor?

      </textarea>
    </section>
  </main>
</body>

</html>
